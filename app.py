import os
import time
import gc
import torch
import requests
import gradio as gr
from huggingface_hub import hf_hub_download, HfApi
from diffusers import (
    AutoPipelineForText2Image, 
    StableDiffusionPipeline,
    StableDiffusionXLPipeline, 
    DPMSolverMultistepScheduler, 
    LCMScheduler,
    EulerDiscreteScheduler 
)

# â”€â”€ 0. CPU æ ¸å¿ƒæ•ˆèƒ½æœ€ä½³åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
torch.set_num_threads(2)

# â”€â”€ 1. è¨­å®šèˆ‡å…¨åŸŸè®Šæ•¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_CACHE_DIR = "./custom_models"
LORA_CACHE_DIR = "./custom_loras"
os.makedirs(MODEL_CACHE_DIR, exist_ok=True)
os.makedirs(LORA_CACHE_DIR, exist_ok=True)

SPACE_ID = os.getenv("SPACE_ID")
ENV_CIVITAI = os.getenv("CIVITAI_TOKEN", "")
ENV_HF = os.getenv("HF_TOKEN", "")

pipe = None
current_model_path = ""
current_model_is_sdxl = False
active_loras = {}  

PRESET_MODELS = {
    "BK-SDM-Tiny (æ¥µé€Ÿè¼•é‡ 1.5)": "nota-ai/bk-sdm-tiny",
    "Stable Diffusion v1.5 (é€šç”¨)": "runwayml/stable-diffusion-v1-5",
    "Dreamlike Anime 1.0 (å‹•æ¼«)": "dreamlike-art/dreamlike-anime-1.0",
    "Kernel NSFW (å¯«å¯¦/æˆäºº)": "Kernel/sd-nsfw",
    "Realistic Vision V5.1 (é«˜ç•«è³ªå¯«å¯¦)": "SG161222/Realistic_Vision_V5.1_noVAE",
    "SDXL 1.0 Base (é«˜ç•«è³ªåº•æ¨¡)": "stabilityai/stable-diffusion-xl-base-1.0", 
}

HF_FILE_MODELS = {
    "HomoSimile XL Pony v6 (ä½ çš„æ¨¡å‹ ğŸ”‘)": ("kines9661/HomoSimile", "homosimileXLPony_v60NAIXLEPSV11.safetensors"),
}

RESOLUTION_CHOICES = [
    384, 448, 512, 576, 640, 704, 768, 832, 896, 960, 1024, 1152, 1280
]

def get_model_choices():
    local_models = [f for f in os.listdir(MODEL_CACHE_DIR) if f.endswith(".safetensors")]
    return list(PRESET_MODELS.keys()) + list(HF_FILE_MODELS.keys()) + local_models

def get_lora_choices():
    return [f for f in os.listdir(LORA_CACHE_DIR) if f.endswith(".safetensors")]

# â”€â”€ 3. æ ¸å¿ƒé‚è¼¯å‡½å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def download_and_backup(url, folder, progress, civit_token="", hf_token=""):
    try:
        headers = {}
        if civit_token and civit_token.strip():
            headers["Authorization"] = f"Bearer {civit_token.strip()}"

        progress(0, desc=f"æ­£åœ¨é€£æ¥...")
        response = requests.get(url, stream=True, headers=headers, timeout=15)
        
        if response.status_code in [401, 403]:
            raise Exception("æ¬Šé™ä¸è¶³ï¼è«‹ç¢ºèª Civitai API Token æ˜¯å¦æ­£ç¢ºã€‚")
        response.raise_for_status()
        
        fname = "temp_download.safetensors"
        if "content-disposition" in response.headers:
            fname = response.headers["content-disposition"].split("filename=")[-1].strip('"')
        else:
            fname = url.split("/")[-1].split("?")[0]
            if not fname.endswith(".safetensors"): fname += ".safetensors"

        local_filepath = os.path.join(folder, fname)
        
        need_download = True
        if os.path.exists(local_filepath) and os.path.getsize(local_filepath) > 1024 * 1024:
            need_download = False

        if need_download:
            total_size = int(response.headers.get("content-length", 0))
            block_size = 1024 * 1024 
            with open(local_filepath, "wb") as f:
                downloaded = 0
                for data in response.iter_content(block_size):
                    f.write(data)
                    downloaded += len(data)
                    if total_size > 0:
                        progress(downloaded / total_size, desc=f"ä¸‹è¼‰ {fname[:20]}: {downloaded/1024/1024:.1f}MB")
            
            if os.path.getsize(local_filepath) < 1024 * 100:
                os.remove(local_filepath)
                raise Exception("æª”æ¡ˆå¤ªå°ï¼Œä¸‹è¼‰å¤±æ•—ã€‚å¯èƒ½æ˜¯ NSFW æ¨¡å‹éœ€æä¾› Tokenã€‚")

        backup_msg = "âœ… (åƒ…æš«å­˜)"
        if SPACE_ID and hf_token and hf_token.strip():
            file_size_mb = os.path.getsize(local_filepath) / (1024 * 1024)
            if file_size_mb > 900:
                backup_msg = "âš ï¸ æª”æ¡ˆ>1GBè·³éå­˜æª”ï¼Œå·²æš«å­˜ã€‚"
                progress(1.0, desc=backup_msg)
            else:
                try:
                    progress(0.9, desc=f"â³ æ­£åœ¨æ°¸ä¹…å‚™ä»½åˆ° Space...")
                    api = HfApi(token=hf_token.strip())
                    repo_path = f"{folder.strip('./')}/{fname}"
                    api.upload_file(
                        path_or_fileobj=local_filepath, path_in_repo=repo_path,
                        repo_id=SPACE_ID, repo_type="space"
                    )
                    backup_msg = "âœ… å·²æ°¸ä¹…å­˜æª”"
                except Exception as upload_err:
                    if "limit reached" in str(upload_err):
                        backup_msg = "âš ï¸ Space é›²ç«¯ç¡¬ç¢Ÿå·²æ»¿ï¼Œè·³éæ°¸ä¹…å­˜æª”ã€‚"
                    else:
                        backup_msg = f"âš ï¸ å­˜æª”å¤±æ•—ï¼Œå·²æš«å­˜ã€‚"
                    progress(1.0, desc=backup_msg)

        return local_filepath, fname, backup_msg
    except Exception as e:
        raise gr.Error(f"ä¸‹è¼‰å¤±æ•—: {str(e)}")


def load_pipeline(model_source, is_local_file=False):
    global pipe, current_model_path, current_model_is_sdxl, active_loras
    if model_source == current_model_path and pipe is not None:
        return f"å·²è¼‰å…¥: {os.path.basename(model_source)}"

    pipe = None
    active_loras = {}
    gc.collect()

    # ã€ä¿®å¾©é‡é» 1ã€‘ï¼šå¼·åˆ¶åˆ¤å®šæ˜¯å¦ç‚º SDXL (å¾æª”åæˆ– Repo å±¬æ€§é›™é‡é©—è­‰)
    is_sdxl_target = False
    source_lower = model_source.lower()
    if "xl" in source_lower or "pony" in source_lower:
        is_sdxl_target = True

    try:
        if is_local_file:
            if is_sdxl_target:
                p = StableDiffusionXLPipeline.from_single_file(
                    model_source, torch_dtype=torch.float32, 
                    safety_checker=None, requires_safety_checker=False, use_safetensors=True
                )
            else:
                # è‹¥ä¸æ˜¯ XL åå­—ï¼Œå…ˆè©¦ SD1.5ï¼Œå¤±æ•—å†ç”¨ SDXL
                try:
                    p = StableDiffusionPipeline.from_single_file(
                        model_source, torch_dtype=torch.float32, 
                        safety_checker=None, requires_safety_checker=False, use_safetensors=True
                    )
                except Exception:
                    p = StableDiffusionXLPipeline.from_single_file(
                        model_source, torch_dtype=torch.float32, 
                        safety_checker=None, requires_safety_checker=False, use_safetensors=True
                    )
                    is_sdxl_target = True
        else:
            p = AutoPipelineForText2Image.from_pretrained(
                model_source, 
                torch_dtype=torch.float32, 
                safety_checker=None, 
                requires_safety_checker=False
            )

        p.to("cpu")
        p.enable_attention_slicing() 
        
        # ã€ä¿®å¾©é‡é» 2ã€‘ï¼šæ ¹æ“šæœ€çµ‚è¼‰å…¥çš„ Pipeline é¡å‹åš´æ ¼åˆ¤å®šæ¶æ§‹
        if isinstance(p, StableDiffusionXLPipeline) or is_sdxl_target:
            current_model_is_sdxl = True
            model_type_str = "SDXL/Pony XL"
        else:
            current_model_is_sdxl = False
            model_type_str = "SD 1.5"

        pipe = p
        current_model_path = model_source
        return f"âœ… æˆåŠŸè¼‰å…¥ ({model_type_str})"
    except Exception as e:
        if is_local_file and os.path.exists(model_source):
            os.remove(model_source) 
        return f"âŒ è¼‰å…¥å¤±æ•—: {str(e)}"

# â”€â”€ 4. UI äº’å‹•äº‹ä»¶è™•ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def handle_model_dropdown(choice, hf_token_val):
    if choice in PRESET_MODELS:
        source = PRESET_MODELS[choice]
        yield "â³ è¼‰å…¥æ¨¡å‹ä¸­ (è‹¥ç‚º SDXL å¯èƒ½éœ€ 2 åˆ†é˜ï¼Œè«‹è€å¿ƒç­‰å¾…)..."
        yield load_pipeline(source, is_local_file=False)

    elif choice in HF_FILE_MODELS:
        repo_id, filename = HF_FILE_MODELS[choice]
        yield f"â³ æ­£åœ¨å¾ HF Hub ä¸‹è¼‰ {filename}... (é¦–æ¬¡éœ€æ™‚è¼ƒé•·)"
        try:
            token = hf_token_val.strip() if hf_token_val and hf_token_val.strip() else None
            local_path = hf_hub_download(
                repo_id=repo_id, 
                filename=filename, 
                token=token,
                local_dir=MODEL_CACHE_DIR
            )
            yield "â³ ä¸‹è¼‰å®Œæˆï¼æ­£åœ¨è¼‰å…¥æ¨¡å‹..."
            yield load_pipeline(local_path, is_local_file=True)
        except Exception as e:
            yield f"âŒ ä¸‹è¼‰å¤±æ•—: {str(e)}ã€‚è‹¥ç‚ºç§äººå€‰åº«è«‹ç¢ºèª HF Token å·²å¡«å…¥ã€‚"
    else:
        source = os.path.join(MODEL_CACHE_DIR, choice)
        yield "â³ è¼‰å…¥æ¨¡å‹ä¸­..."
        yield load_pipeline(source, is_local_file=True)

def handle_civitai_model_download(url, civit_token, hf_token, progress=gr.Progress()):
    if not url: 
        yield "âŒ è«‹è¼¸å…¥ç¶²å€", gr.update()
        return
    yield "â³ ä¸‹è¼‰èˆ‡è™•ç†ä¸­...", gr.update()
    try:
        path, fname, backup_msg = download_and_backup(url, MODEL_CACHE_DIR, progress, civit_token, hf_token)
        yield f"â³ è¼‰å…¥æ¨¡å‹ä¸­... ({backup_msg})", gr.update()
        status = load_pipeline(path, True)
        choices = get_model_choices()
        yield f"{status} | {backup_msg}", gr.update(choices=choices, value=fname)
    except Exception as e:
        yield f"âŒ éŒ¯èª¤: {e}", gr.update()

def update_lora_list_text():
    if not active_loras: return "ç„¡"
    return "\n".join([f"- {k}: {v}" for k, v in active_loras.items()])

def handle_lora_dropdown(lora_filename, scale):
    global pipe, active_loras
    if pipe is None: return "âš ï¸ è«‹å…ˆè¼‰å…¥ä¸»æ¨¡å‹", update_lora_list_text()
    if not lora_filename: return "âš ï¸ æœªé¸æ“‡ LoRA", update_lora_list_text()
    path = os.path.join(LORA_CACHE_DIR, lora_filename)
    adapter_name = lora_filename.replace(".", "_")
    try:
        pipe.load_lora_weights(path, adapter_name=adapter_name)
        active_loras[adapter_name] = float(scale)
        return f"âœ… å·²åŠ å…¥: {lora_filename}", update_lora_list_text()
    except Exception as e:
        error_msg = str(e)
        if "size mismatch" in error_msg or "No modules were targeted" in error_msg:
            return f"âŒ æ¶æ§‹ä¸ç¬¦ï¼LoRA èˆ‡ä¸»æ¨¡å‹ä¸ç›¸å®¹ã€‚", update_lora_list_text()
        return f"âŒ LoRA è¼‰å…¥å¤±æ•—: {error_msg}", update_lora_list_text()

def handle_lora_download(url, scale, civit_token, hf_token, progress=gr.Progress()):
    global pipe, active_loras
    if pipe is None: return "âš ï¸ è«‹å…ˆè¼‰å…¥ä¸»æ¨¡å‹", update_lora_list_text(), gr.update()
    try:
        path, fname, backup_msg = download_and_backup(url, LORA_CACHE_DIR, progress, civit_token, hf_token)
        adapter_name = fname.replace(".", "_")
        try:
            pipe.load_lora_weights(path, adapter_name=adapter_name)
            active_loras[adapter_name] = float(scale)
            choices = get_lora_choices()
            return f"âœ… å·²å¥—ç”¨ {fname} | {backup_msg}", update_lora_list_text(), gr.update(choices=choices, value=fname)
        except Exception as e:
            if adapter_name in active_loras: del active_loras[adapter_name]
            error_msg = str(e)
            if "size mismatch" in error_msg or "No modules were targeted" in error_msg:
                return f"âŒ æ¶æ§‹ä¸ç¬¦ï¼LoRA èˆ‡ä¸»æ¨¡å‹ä¸ç›¸å®¹ã€‚", update_lora_list_text(), gr.update()
            return f"âŒ LoRA è¼‰å…¥å¤±æ•—: {error_msg}", update_lora_list_text(), gr.update()
    except Exception as e:
        return f"âŒ éŒ¯èª¤: {e}", update_lora_list_text(), gr.update()

def clear_loras():
    global active_loras
    if pipe is None: return "âš ï¸ ç„¡æ¨¡å‹"
    active_loras = {}
    return "ğŸ—‘ï¸ å·²ç§»é™¤æ‰€æœ‰è‡ªè¨‚ LoRA"

# â”€â”€ 5. ç”Ÿæˆåœ–ç‰‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_image(prompt, neg, steps, cfg, seed, width, height, use_lcm):
    if pipe is None: raise gr.Error("è«‹å…ˆè¼‰å…¥æ¨¡å‹ï¼")

    start_time = time.time()
    if seed == -1: seed = int(time.time() % (2**32))
    generator = torch.Generator("cpu").manual_seed(seed)
    
    adapters_to_use = []
    weights_to_use = []
    pipe.unload_lora_weights()
    pipe.disable_lora()
    warning_msg = ""

    # ã€ä¿®å¾©é‡é» 3ã€‘ï¼šæ›´ç²¾æº–çš„åŠ é€Ÿ LoRA åˆ†é…é‚è¼¯
    if use_lcm:
        if current_model_is_sdxl:
            # ç¢ºèªç‚º SDXL / Pony æ¨¡å‹ï¼Œæ›è¼‰ SDXL å°ˆç”¨ Lightning LoRA
            try:
                pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing="trailing")
                lightning_ckpt = hf_hub_download("ByteDance/SDXL-Lightning", "sdxl_lightning_4step_lora.safetensors")
                pipe.load_lora_weights(lightning_ckpt, adapter_name="lightning")
                adapters_to_use.append("lightning")
                weights_to_use.append(1.0)
                warning_msg = "âš¡ SDXL Lightning å·²å•Ÿå‹•ã€‚å»ºè­° Steps=4~8, CFG=1.0~2.0ã€‚ "
            except Exception as e:
                warning_msg = f"âš ï¸ Lightning è¼‰å…¥å¤±æ•— ({str(e)[:50]})ï¼Œé€€å›ä¸€èˆ¬æ¨¡å¼ã€‚ "
                pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
        else:
            # ç¢ºèªç‚º SD1.5 æ¨¡å‹ï¼Œæ›è¼‰ SD1.5 å°ˆç”¨ LCM LoRA
            try:
                pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)
                pipe.load_lora_weights("latent-consistency/lcm-lora-sdv1-5", adapter_name="lcm")
                adapters_to_use.append("lcm")
                weights_to_use.append(1.0)
                warning_msg = "âš¡ LCM å·²å•Ÿå‹•ã€‚å»ºè­° Steps=4~8, CFG=1.0~2.0ã€‚ "
            except Exception as e:
                warning_msg = f"âš ï¸ LCM è¼‰å…¥å¤±æ•— ({str(e)[:50]})ï¼Œé€€å›ä¸€èˆ¬æ¨¡å¼ã€‚ "
                pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
    else:
        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)

    for k, v in active_loras.items():
        try:
            lora_filename = k.replace("_", ".")
            path = os.path.join(LORA_CACHE_DIR, lora_filename)
            pipe.load_lora_weights(path, adapter_name=k)
            adapters_to_use.append(k)
            weights_to_use.append(v)
        except Exception:
            pass 

    if len(adapters_to_use) > 0:
        pipe.enable_lora()
        pipe.set_adapters(adapters_to_use, adapter_weights=weights_to_use)

    # ç”Ÿæˆå½±åƒ
    image = pipe(
        prompt=prompt, 
        negative_prompt=neg if not use_lcm else None,
        num_inference_steps=int(steps), 
        guidance_scale=float(cfg), 
        width=int(width), height=int(height),
        generator=generator
    ).images[0]
    
    cost_time = time.time() - start_time
    return image, warning_msg + f"âœ… å®Œæˆ | {width}x{height} | è€—æ™‚: {cost_time:.1f}s | Seed: {seed}"


# â”€â”€ 6. Gradio UI ä»‹é¢è¨­è¨ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with gr.Blocks(title="Turbo CPU SD + æ°¸ä¹…åœ–åº«") as demo:
    gr.Markdown("# âš¡ Turbo CPU SD (NSFW + SDXL/Pony æ”¯æ´)")
    
    with gr.Row():
        with gr.Column(scale=1):
            with gr.Accordion("âš™ï¸ æˆæ¬Šé‡‘é‘°è¨­å®š (å·²è‡ªå‹•å¸¶å…¥)", open=False):
                civit_token = gr.Textbox(label="Civitai API Token", value=ENV_CIVITAI, placeholder="ä¸‹è¼‰ NSFW æ¨¡å‹ç”¨", type="password")
                hf_token = gr.Textbox(label="HF Write Token", value=ENV_HF, placeholder="æ°¸ä¹…å‚™ä»½ + ç§äººæ¨¡å‹ç”¨", type="password")
            
            gr.Markdown("### 1. ä¸»æ¨¡å‹ç®¡ç†")
            with gr.Tabs():
                with gr.TabItem("ğŸ—‚ï¸ é¸æ“‡åœ–åº«æ¨¡å‹"):
                    model_dropdown = gr.Dropdown(choices=get_model_choices(), value=get_model_choices()[0], label="é¸æ“‡æ¨¡å‹", interactive=True)
                    load_model_btn = gr.Button("è¼‰å…¥é¸æ“‡çš„æ¨¡å‹", variant="primary")
                with gr.TabItem("ğŸŒ ä¸‹è¼‰æ–°æ¨¡å‹"):
                    civit_ckpt_url = gr.Textbox(label="Checkpoint ç¶²å€", placeholder="è¼¸å…¥ Civitai ç›´é€£...")
                    download_model_btn = gr.Button("ä¸‹è¼‰ã€å‚™ä»½ä¸¦è¼‰å…¥")
            
            model_status = gr.Textbox(label="ç³»çµ±ç‹€æ…‹", value="æœªè¼‰å…¥", interactive=False)

            gr.Markdown("### 2. LoRA ç®¡ç†")
            lora_scale = gr.Slider(0.1, 2.0, value=0.8, step=0.05, label="LoRA æ¬Šé‡ (Scale)")
            with gr.Tabs():
                with gr.TabItem("ğŸ—‚ï¸ é¸æ“‡åœ–åº« LoRA"):
                    lora_dropdown = gr.Dropdown(choices=get_lora_choices(), label="é¸æ“‡å·²å‚™ä»½çš„ LoRA", interactive=True)
                    load_lora_btn = gr.Button("â• å¥—ç”¨æ­¤ LoRA")
                with gr.TabItem("ğŸŒ ä¸‹è¼‰æ–° LoRA"):
                    lora_url = gr.Textbox(label="LoRA ä¸‹è¼‰ç¶²å€", placeholder="è¼¸å…¥ Civitai ç›´é€£...")
                    download_lora_btn = gr.Button("â• ä¸‹è¼‰ã€å‚™ä»½ä¸¦å¥—ç”¨")
            
            clear_lora_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰å·²å¥—ç”¨çš„ LoRA")
            lora_status = gr.Textbox(label="ç›®å‰å·²å¥—ç”¨æ¸…å–®", value="ç„¡", lines=2, interactive=False)

        with gr.Column(scale=2):
            use_lcm = gr.Checkbox(label="âš¡ å•Ÿç”¨æ¥µé€Ÿæ¨¡å¼ (SD1.5â†’LCM / SDXLâ†’Lightning)", value=True)
            
            gr.Markdown("ğŸ’¡ **Pony XL ä½¿ç”¨æç¤º**ï¼šPrompt é–‹é ­è«‹åŠ  `score_9, score_8_up, score_7_up,`")
            prompt = gr.Textbox(label="Prompt", value="score_9, score_8_up, score_7_up, a beautiful woman, masterpiece", lines=3)
            neg = gr.Textbox(label="Negative Prompt (æ¥µé€Ÿæ¨¡å¼ä¸‹å°‡å¿½ç•¥)", value="score_1, score_2, score_3, low quality, bad anatomy, worst quality", lines=1)
            
            with gr.Row():
                steps = gr.Slider(1, 30, value=5, step=1, label="Steps (æ¥µé€Ÿæ¨¡å¼å»ºè­° 4~8)")
                cfg = gr.Slider(1.0, 10.0, value=5.0, step=0.5, label="CFG (Pony å»ºè­° 5~7)")
                seed = gr.Number(-1, label="Seed (-1=éš¨æ©Ÿ)", precision=0)
            
            gr.Markdown("*(SD 1.5 å»ºè­° 512~768ï¼›SDXL/Pony å»ºè­° 1024)*")
            with gr.Row():
                width = gr.Dropdown(RESOLUTION_CHOICES, value=1024, label="Width")
                height = gr.Dropdown(RESOLUTION_CHOICES, value=1024, label="Height")
                
            gen_btn = gr.Button("âœ¨ ç”Ÿæˆåœ–ç‰‡", variant="primary", size="lg")
            gen_status = gr.Textbox(label="ç”Ÿæˆç‹€æ…‹", interactive=False)
            out_img = gr.Image(label="ç”Ÿæˆçµæœ", type="pil")

    # â”€â”€ 7. ç¶å®šæŒ‰éˆ•äº‹ä»¶ â”€â”€
    load_model_btn.click(fn=handle_model_dropdown, inputs=[model_dropdown, hf_token], outputs=[model_status])
    download_model_btn.click(fn=handle_civitai_model_download, inputs=[civit_ckpt_url, civit_token, hf_token], outputs=[model_status, model_dropdown])
    
    load_lora_btn.click(fn=handle_lora_dropdown, inputs=[lora_dropdown, lora_scale], outputs=[model_status, lora_status])
    download_lora_btn.click(fn=handle_lora_download, inputs=[lora_url, lora_scale, civit_token, hf_token], outputs=[model_status, lora_status, lora_dropdown])
    clear_lora_btn.click(fn=clear_loras, outputs=[model_status]).then(fn=update_lora_list_text, outputs=[lora_status])
    
    gen_btn.click(fn=generate_image, inputs=[prompt, neg, steps, cfg, seed, width, height, use_lcm], outputs=[out_img, gen_status])

demo.queue().launch()
