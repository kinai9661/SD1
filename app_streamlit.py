import os
import streamlit as st
import torch
import requests
from tqdm import tqdm
from diffusers import (
    StableDiffusionPipeline,
    StableDiffusionXLPipeline,
    LCMScheduler,
    EulerDiscreteScheduler,
    DPMSolverMultistepScheduler,
)
from diffusers.utils import load_image
from huggingface_hub import hf_hub_download, login
from PIL import Image

# ============== è¨­å®š ==============
MODEL_CACHE_DIR = "./models"
LORA_CACHE_DIR = "./loras"
HF_TOKEN = os.getenv("HF_TOKEN", "")
CIVIT_TOKEN = os.getenv("CIVIT_TOKEN", "")

os.makedirs(MODEL_CACHE_DIR, exist_ok=True)
os.makedirs(LORA_CACHE_DIR, exist_ok=True)

# ============== é è¨­æ¨¡å‹ ==============
PRESET_MODELS = {
    "BK-SDM-Tiny (æ¥µé€Ÿè¼•é‡ 1.5)": "nota-ai/bk-sdm-tiny",
    "Stable Diffusion v1.5 (é€šç”¨)": "runwayml/stable-diffusion-v1-5",
    "Dreamlike Anime 1.0 (å‹•æ¼«)": "dreamlike-art/dreamlike-anime-1.0",
    "Kernel NSFW (å¯«å¯¦/æˆäºº)": "Kernel/sd-nsfw",
    "Realistic Vision V5.1 (é«˜ç•«è³ªå¯«å¯¦)": "SG161222/Realistic_Vision_V5.1_noVAE",
    "SDXL 1.0 Base (é«˜ç•«è³ªåº•æ¨¡)": "stabilityai/stable-diffusion-xl-base-1.0",
}

HF_FILE_MODELS = {
    "SDXL Lightning (æ¥µé€Ÿ SDXL)": ("ByteDance/SDXL-Lightning", "sdxl_lightning_4step_lora.safetensors"),
    "Pony Diffusion XL V6 (å‹•æ¼«/æˆäºº)": ("PonyXL_v6", "ponyxl_v6.safetensors"),
}

# ============== Session State åˆå§‹åŒ– ==============
def init_session_state():
    """åˆå§‹åŒ–æ‰€æœ‰ session state è®Šæ•¸"""
    if "pipe" not in st.session_state:
        st.session_state.pipe = None
    if "current_model_path" not in st.session_state:
        st.session_state.current_model_path = ""
    if "current_model_is_sdxl" not in st.session_state:
        st.session_state.current_model_is_sdxl = False
    if "active_loras" not in st.session_state:
        st.session_state.active_loras = {}
    if "generated_images" not in st.session_state:
        st.session_state.generated_images = []
    if "status_message" not in st.session_state:
        st.session_state.status_message = ""
    if "hf_token" not in st.session_state:
        st.session_state.hf_token = HF_TOKEN
    if "civit_token" not in st.session_state:
        st.session_state.civit_token = CIVIT_TOKEN

# ============== ä¸‹è¼‰å‡½æ•¸ ==============
def download_and_backup(url, folder, civit_token="", hf_token=""):
    """ä¸‹è¼‰æª”æ¡ˆä¸¦å‚™ä»½"""
    headers = {}
    if "civitai.com" in url and civit_token:
        headers["Authorization"] = f"Bearer {civit_token}"
    
    response = requests.get(url, headers=headers, stream=True, timeout=60)
    response.raise_for_status()
    
    # å˜—è©¦å¾ header å–å¾—æª”å
    content_disp = response.headers.get("content-disposition", "")
    filename = "downloaded_model.safetensors"
    if "filename=" in content_disp:
        filename = content_disp.split("filename=")[1].strip('"')
    
    local_filepath = os.path.join(folder, filename)
    
    # ä¸‹è¼‰é€²åº¦
    total_size = int(response.headers.get("content-length", 0))
    progress_bar = st.progress(0, text=f"ä¸‹è¼‰ä¸­: {filename}")
    
    with open(local_filepath, "wb") as f:
        downloaded = 0
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                downloaded += len(chunk)
                if total_size > 0:
                    progress = downloaded / total_size
                    progress_bar.progress(progress, text=f"ä¸‹è¼‰ä¸­: {filename} ({int(progress*100)}%)")
    
    progress_bar.empty()
    return local_filepath, filename, "âœ… ä¸‹è¼‰å®Œæˆ"

# ============== è¼‰å…¥æ¨¡å‹ ==============
@st.cache_resource
def load_pipeline_cached(model_source, is_local_file=False, hf_token=""):
    """å¿«å–æ¨¡å‹è¼‰å…¥"""
    is_sdxl = False
    
    if is_local_file:
        if "sdxl" in model_source.lower() or "xl" in model_source.lower():
            pipe = StableDiffusionXLPipeline.from_single_file(
                model_source,
                torch_dtype=torch.float32,
                use_safetensors=True,
            )
            is_sdxl = True
        else:
            pipe = StableDiffusionPipeline.from_single_file(
                model_source,
                torch_dtype=torch.float32,
                use_safetensors=True,
            )
    else:
        # åˆ¤æ–·æ˜¯å¦ç‚º SDXL
        if "sdxl" in model_source.lower() or "xl" in model_source.lower():
            pipe = StableDiffusionXLPipeline.from_pretrained(
                model_source,
                torch_dtype=torch.float32,
                use_auth_token=hf_token if hf_token else None,
            )
            is_sdxl = True
        else:
            pipe = StableDiffusionPipeline.from_pretrained(
                model_source,
                torch_dtype=torch.float32,
                use_auth_token=hf_token if hf_token else None,
            )
    
    pipe.to("cpu")
    pipe.safety_checker = None
    pipe.requires_safety_checker = False
    
    return pipe, model_source, is_sdxl

def load_pipeline(model_source, is_local_file=False):
    """è¼‰å…¥æ¨¡å‹ç®¡ç·š"""
    try:
        pipe, path, is_sdxl = load_pipeline_cached(
            model_source, 
            is_local_file, 
            st.session_state.hf_token
        )
        st.session_state.pipe = pipe
        st.session_state.current_model_path = path
        st.session_state.current_model_is_sdxl = is_sdxl
        st.session_state.active_loras = {}
        return f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ: {model_source}"
    except Exception as e:
        return f"âŒ è¼‰å…¥å¤±æ•—: {str(e)}"

# ============== æ¨¡å‹è™•ç† ==============
def handle_model_dropdown(choice):
    """è™•ç†é è¨­æ¨¡å‹é¸æ“‡"""
    if not choice:
        return "è«‹é¸æ“‡æ¨¡å‹"
    
    model_id = PRESET_MODELS.get(choice)
    if model_id:
        return load_pipeline(model_id)
    return "âŒ æœªçŸ¥çš„æ¨¡å‹é¸æ“‡"

def handle_hf_file_model(choice):
    """è™•ç† HF æª”æ¡ˆæ¨¡å‹"""
    if not choice:
        return "è«‹é¸æ“‡æ¨¡å‹"
    
    repo_id, filename = HF_FILE_MODELS.get(choice, (None, None))
    if repo_id and filename:
        try:
            filepath = hf_hub_download(
                repo_id=repo_id,
                filename=filename,
                cache_dir=MODEL_CACHE_DIR,
                token=st.session_state.hf_token
            )
            return load_pipeline(filepath, is_local_file=True)
        except Exception as e:
            return f"âŒ ä¸‹è¼‰å¤±æ•—: {str(e)}"
    return "âŒ æœªçŸ¥çš„æ¨¡å‹é¸æ“‡"

def handle_civitai_model_download(url):
    """è™•ç† Civitai æ¨¡å‹ä¸‹è¼‰"""
    if not url:
        return "è«‹è¼¸å…¥ Civitai æ¨¡å‹ URL"
    
    try:
        path, fname, msg = download_and_backup(
            url, MODEL_CACHE_DIR, 
            st.session_state.civit_token, 
            st.session_state.hf_token
        )
        result = load_pipeline(path, is_local_file=True)
        return f"{msg}\n{result}"
    except Exception as e:
        return f"âŒ ä¸‹è¼‰å¤±æ•—: {str(e)}"

# ============== LoRA è™•ç† ==============
def get_available_loras():
    """å–å¾—å¯ç”¨çš„ LoRA åˆ—è¡¨"""
    loras = []
    for folder in [LORA_CACHE_DIR, "./custom_loras"]:
        if os.path.exists(folder):
            for f in os.listdir(folder):
                if f.endswith((".safetensors", ".bin")):
                    loras.append(os.path.join(folder, f))
    return loras

def handle_lora_dropdown(lora_path, scale):
    """è™•ç† LoRA è¼‰å…¥"""
    if not lora_path or not os.path.exists(lora_path):
        return "è«‹é¸æ“‡æœ‰æ•ˆçš„ LoRA"
    
    if st.session_state.pipe is None:
        return "âŒ è«‹å…ˆè¼‰å…¥æ¨¡å‹"
    
    try:
        st.session_state.pipe.load_lora_weights(
            os.path.dirname(lora_path),
            weight_name=os.path.basename(lora_path),
            cross_attention_scale=scale
        )
        st.session_state.active_loras[lora_path] = scale
        return f"âœ… LoRA è¼‰å…¥æˆåŠŸ: {os.path.basename(lora_path)}"
    except Exception as e:
        return f"âŒ LoRA è¼‰å…¥å¤±æ•—: {str(e)}"

def handle_lora_download(url, scale):
    """è™•ç† LoRA ä¸‹è¼‰"""
    if not url:
        return "è«‹è¼¸å…¥ LoRA URL"
    
    if st.session_state.pipe is None:
        return "âŒ è«‹å…ˆè¼‰å…¥æ¨¡å‹"
    
    try:
        path, fname, msg = download_and_backup(
            url, LORA_CACHE_DIR,
            st.session_state.civit_token,
            st.session_state.hf_token
        )
        result = handle_lora_dropdown(path, scale)
        return f"{msg}\n{result}"
    except Exception as e:
        return f"âŒ ä¸‹è¼‰å¤±æ•—: {str(e)}"

def clear_loras():
    """æ¸…é™¤æ‰€æœ‰ LoRA"""
    if st.session_state.pipe is None:
        return "æ²’æœ‰è¼‰å…¥çš„æ¨¡å‹"
    
    try:
        st.session_state.pipe.unload_lora_weights()
        st.session_state.active_loras = {}
        return "âœ… å·²æ¸…é™¤æ‰€æœ‰ LoRA"
    except Exception as e:
        return f"âŒ æ¸…é™¤å¤±æ•—: {str(e)}"

# ============== åœ–ç‰‡ç”Ÿæˆ ==============
def generate_image(prompt, neg_prompt, steps, cfg, seed, width, height, use_lcm):
    """ç”Ÿæˆåœ–ç‰‡"""
    if st.session_state.pipe is None:
        return None, "âŒ è«‹å…ˆè¼‰å…¥æ¨¡å‹"
    
    try:
        # è¨­å®š scheduler
        if use_lcm:
            if st.session_state.current_model_is_sdxl:
                # SDXL Lightning
                st.session_state.pipe.scheduler = EulerDiscreteScheduler.from_config(
                    st.session_state.pipe.scheduler.config,
                    timestep_spacing="trailing",
                )
            else:
                # SD 1.5 LCM
                st.session_state.pipe.scheduler = LCMScheduler.from_config(
                    st.session_state.pipe.scheduler.config
                )
        else:
            st.session_state.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                st.session_state.pipe.scheduler.config
            )
        
        # ç”Ÿæˆåƒæ•¸
        generator = torch.Generator("cpu").manual_seed(seed)
        
        # åŸ·è¡Œç”Ÿæˆ
        with st.spinner("ç”Ÿæˆä¸­..."):
            result = st.session_state.pipe(
                prompt=prompt,
                negative_prompt=neg_prompt,
                num_inference_steps=steps,
                guidance_scale=cfg,
                width=width,
                height=height,
                generator=generator,
            )
        
        image = result.images[0]
        
        # å„²å­˜åœ–ç‰‡
        os.makedirs("./outputs", exist_ok=True)
        timestamp = seed
        save_path = f"./outputs/image_{timestamp}.png"
        image.save(save_path)
        
        # åŠ å…¥åˆ°å·²ç”Ÿæˆåˆ—è¡¨
        st.session_state.generated_images.append({
            "image": image,
            "path": save_path,
            "prompt": prompt,
            "seed": seed
        })
        
        return image, f"âœ… ç”Ÿæˆå®Œæˆ! å·²å„²å­˜è‡³ {save_path}"
        
    except Exception as e:
        return None, f"âŒ ç”Ÿæˆå¤±æ•—: {str(e)}"

# ============== ä¸»ä»‹é¢ ==============
def main():
    st.set_page_config(
        page_title="Turbo CPU SD + æ°¸ä¹…åœ–åº«",
        page_icon="âš¡",
        layout="wide"
    )
    
    # åˆå§‹åŒ– session state
    init_session_state()
    
    # æ¨™é¡Œ
    st.title("âš¡ Turbo CPU SD (NSFW + SDXL/Pony æ”¯æ´)")
    st.markdown("---")
    
    # å´é‚Šæ¬„ - æˆæ¬Šè¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ æˆæ¬Šé‡‘é‘°è¨­å®š")
        st.session_state.hf_token = st.text_input(
            "HF Token", 
            value=st.session_state.hf_token,
            type="password"
        )
        st.session_state.civit_token = st.text_input(
            "Civitai Token", 
            value=st.session_state.civit_token,
            type="password"
        )
        
        st.markdown("---")
        st.header("ğŸ“Š ç‹€æ…‹")
        if st.session_state.current_model_path:
            st.success(f"ç›®å‰æ¨¡å‹: {st.session_state.current_model_path}")
        else:
            st.warning("å°šæœªè¼‰å…¥æ¨¡å‹")
        
        if st.session_state.active_loras:
            st.info(f"å·²è¼‰å…¥ LoRA: {len(st.session_state.active_loras)} å€‹")
        
        st.markdown("---")
        st.header("ğŸ–¼ï¸ åœ–åº«")
        if st.session_state.generated_images:
            for img_data in reversed(st.session_state.generated_images[-5:]):
                st.image(img_data["image"], caption=f"Seed: {img_data['seed']}", use_container_width=True)
        else:
            st.write("å°šç„¡ç”Ÿæˆçš„åœ–ç‰‡")
    
    # ä¸»è¦å…§å®¹å€
    col_left, col_right = st.columns([1, 2])
    
    # å·¦å´ - æ¨¡å‹èˆ‡ LoRA æ§åˆ¶
    with col_left:
        # æ¨¡å‹é¸æ“‡ Tabs
        tab1, tab2, tab3 = st.tabs(["ğŸ“¦ é è¨­æ¨¡å‹", "ğŸ“ HF æª”æ¡ˆæ¨¡å‹", "ğŸ”— Civitai ä¸‹è¼‰"])
        
        with tab1:
            st.subheader("é è¨­æ¨¡å‹")
            preset_choice = st.selectbox(
                "é¸æ“‡é è¨­æ¨¡å‹",
                options=[""] + list(PRESET_MODELS.keys()),
                key="preset_model_select"
            )
            if st.button("è¼‰å…¥é è¨­æ¨¡å‹", key="load_preset"):
                with st.spinner("è¼‰å…¥ä¸­..."):
                    st.session_state.status_message = handle_model_dropdown(preset_choice)
        
        with tab2:
            st.subheader("HF æª”æ¡ˆæ¨¡å‹")
            hf_choice = st.selectbox(
                "é¸æ“‡ HF æª”æ¡ˆæ¨¡å‹",
                options=[""] + list(HF_FILE_MODELS.keys()),
                key="hf_model_select"
            )
            if st.button("è¼‰å…¥ HF æ¨¡å‹", key="load_hf"):
                with st.spinner("è¼‰å…¥ä¸­..."):
                    st.session_state.status_message = handle_hf_file_model(hf_choice)
        
        with tab3:
            st.subheader("Civitai ä¸‹è¼‰")
            civit_url = st.text_input("Civitai æ¨¡å‹ URL", key="civit_url")
            if st.button("ä¸‹è¼‰ä¸¦è¼‰å…¥", key="download_civit"):
                if civit_url:
                    st.session_state.status_message = handle_civitai_model_download(civit_url)
        
        st.markdown("---")
        
        # LoRA æ§åˆ¶
        st.subheader("ğŸ¨ LoRA æ§åˆ¶")
        
        lora_tabs1, lora_tabs2 = st.tabs(["æœ¬åœ° LoRA", "ä¸‹è¼‰ LoRA"])
        
        with lora_tabs1:
            available_loras = get_available_loras()
            lora_choice = st.selectbox(
                "é¸æ“‡æœ¬åœ° LoRA",
                options=[""] + available_loras,
                format_func=lambda x: os.path.basename(x) if x else "",
                key="lora_select"
            )
            lora_scale = st.slider("LoRA å¼·åº¦", 0.0, 2.0, 1.0, 0.1, key="lora_scale")
            if st.button("è¼‰å…¥ LoRA", key="load_lora"):
                st.session_state.status_message = handle_lora_dropdown(lora_choice, lora_scale)
        
        with lora_tabs2:
            lora_url = st.text_input("LoRA URL", key="lora_url")
            lora_dl_scale = st.slider("ä¸‹è¼‰ LoRA å¼·åº¦", 0.0, 2.0, 1.0, 0.1, key="lora_dl_scale")
            if st.button("ä¸‹è¼‰ä¸¦è¼‰å…¥ LoRA", key="download_lora"):
                st.session_state.status_message = handle_lora_download(lora_url, lora_dl_scale)
        
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰ LoRA", key="clear_loras"):
            st.session_state.status_message = clear_loras()
        
        # é¡¯ç¤ºç‹€æ…‹è¨Šæ¯
        if st.session_state.status_message:
            st.markdown("---")
            st.info(st.session_state.status_message)
    
    # å³å´ - ç”Ÿæˆæ§åˆ¶
    with col_right:
        st.subheader("ğŸ–¼ï¸ åœ–ç‰‡ç”Ÿæˆ")
        
        # æç¤ºè©
        prompt = st.text_area(
            "æ­£å‘æç¤ºè© (Prompt)",
            height=100,
            placeholder="è¼¸å…¥æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„åœ–ç‰‡å…§å®¹..."
        )
        
        neg_prompt = st.text_area(
            "è² å‘æç¤ºè© (Negative Prompt)",
            value="lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry",
            height=80
        )
        
        # ç”Ÿæˆåƒæ•¸
        col_param1, col_param2, col_param3 = st.columns(3)
        
        with col_param1:
            steps = st.slider("æ­¥æ•¸ (Steps)", 1, 50, 4 if True else 20, key="steps")
            cfg = st.slider("CFG Scale", 1.0, 20.0, 2.0, 0.5, key="cfg")
        
        with col_param2:
            width = st.select_slider(
                "å¯¬åº¦",
                options=[384, 512, 640, 768, 896, 1024, 1152, 1280],
                value=512,
                key="width"
            )
            height = st.select_slider(
                "é«˜åº¦",
                options=[384, 512, 640, 768, 896, 1024, 1152, 1280],
                value=512,
                key="height"
            )
        
        with col_param3:
            seed = st.number_input("ç¨®å­ (Seed)", -1, 999999999, -1, key="seed")
            if seed == -1:
                import random
                seed = random.randint(0, 999999999)
            
            use_lcm = st.checkbox(
                "âš¡ å•Ÿç”¨æ¥µé€Ÿæ¨¡å¼ (SD1.5â†’LCM / SDXLâ†’Lightning)",
                value=True,
                key="use_lcm"
            )
        
        # ç”ŸæˆæŒ‰éˆ•
        if st.button("ğŸ¨ ç”Ÿæˆåœ–ç‰‡", type="primary", use_container_width=True):
            if not prompt:
                st.error("è«‹è¼¸å…¥æç¤ºè©")
            else:
                image, message = generate_image(
                    prompt, neg_prompt, steps, cfg, seed, width, height, use_lcm
                )
                if image:
                    st.success(message)
                    st.image(image, caption=f"Seed: {seed}", use_container_width=True)
                else:
                    st.error(message)
        
        # é¡¯ç¤ºå·²ç”Ÿæˆçš„åœ–ç‰‡
        if st.session_state.generated_images:
            st.markdown("---")
            st.subheader("ğŸ“¸ å·²ç”Ÿæˆåœ–ç‰‡")
            for img_data in reversed(st.session_state.generated_images):
                with st.expander(f"Seed: {img_data['seed']} - {img_data['prompt'][:50]}..."):
                    st.image(img_data["image"], use_container_width=True)
                    st.caption(f"æç¤ºè©: {img_data['prompt']}")

if __name__ == "__main__":
    main()
