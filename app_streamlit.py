import os
import gc
import json
import glob
import streamlit as st
import torch
import requests
from datetime import datetime
from tqdm import tqdm
from dotenv import load_dotenv
from diffusers import (
    StableDiffusionPipeline,
    StableDiffusionXLPipeline,
    DPMSolverMultistepScheduler,
)
from diffusers.utils import load_image
from huggingface_hub import hf_hub_download, login, HfApi, list_models, snapshot_download
from PIL import Image

# ============== ç’°å¢ƒè®Šæ•¸è¼‰å…¥ ==============
load_dotenv()  # è¼‰å…¥ .env æª”æ¡ˆ

def get_secret(key, default=""):
    """å¾ st.secrets æˆ–ç’°å¢ƒè®Šæ•¸å–å¾—è¨­å®šå€¼"""
    try:
        if hasattr(st, 'secrets') and key in st.secrets:
            return st.secrets[key]
    except:
        pass
    return os.getenv(key, default)

# ============== è¨­å®š ==============
MODEL_CACHE_DIR = "./models"
LORA_CACHE_DIR = "./loras"
OUTPUT_DIR = "./outputs"
HISTORY_DIR = "./history"
HF_TOKEN = get_secret("HF_TOKEN", "")
CIVIT_TOKEN = get_secret("CIVIT_TOKEN", "")
DEFAULT_STEPS = int(get_secret("DEFAULT_STEPS", "20"))
DEFAULT_CFG = float(get_secret("DEFAULT_CFG", "7.0"))
MAX_HISTORY_IMAGES = int(get_secret("MAX_HISTORY_IMAGES", "100"))

os.makedirs(MODEL_CACHE_DIR, exist_ok=True)
os.makedirs(LORA_CACHE_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(HISTORY_DIR, exist_ok=True)

# ============== é è¨­æ¨¡å‹ ==============
PRESET_MODELS = {
    "BK-SDM-Tiny (æ¥µé€Ÿè¼•é‡ 1.5)": "nota-ai/bk-sdm-tiny",
    "Stable Diffusion v1.5 (é€šç”¨)": "runwayml/stable-diffusion-v1-5",
    "Dreamlike Anime 1.0 (å‹•æ¼«)": "dreamlike-art/dreamlike-anime-1.0",
    "Kernel NSFW (å¯«å¯¦/æˆäºº)": "Kernel/sd-nsfw",
    "Realistic Vision V5.1 (é«˜ç•«è³ªå¯«å¯¦)": "SG161222/Realistic_Vision_V5.1_noVAE",
    "SDXL 1.0 Base (é«˜ç•«è³ªåº•æ¨¡)": "stabilityai/stable-diffusion-xl-base-1.0",
}

HF_FILE_MODELS = {
    "SDXL Lightning (æ¥µé€Ÿ SDXL)": ("ByteDance/SDXL-Lightning", "sdxl_lightning_4step_lora.safetensors"),
    "Pony Diffusion XL V6 (å‹•æ¼«/æˆäºº)": ("PonyXL_v6", "ponyxl_v6.safetensors"),
}

# ============== è¨˜æ†¶é«”ç®¡ç† ==============
def clear_memory():
    """æ¸…ç†è¨˜æ†¶é«”"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

def optimize_pipeline(pipe):
    """å„ªåŒ–ç®¡ç·šä»¥é©æ‡‰ CPU/ä½è¨˜æ†¶é«”ç’°å¢ƒ"""
    # å•Ÿç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
    if hasattr(pipe, 'enable_attention_slicing'):
        pipe.enable_attention_slicing()
    # å•Ÿç”¨ VAE åˆ‡ç‰‡
    if hasattr(pipe, 'enable_vae_slicing'):
        pipe.enable_vae_slicing()
    return pipe

# ============== Session State åˆå§‹åŒ– ==============
def init_session_state():
    """åˆå§‹åŒ–æ‰€æœ‰ session state è®Šæ•¸"""
    if "pipe" not in st.session_state:
        st.session_state.pipe = None
    if "current_model_path" not in st.session_state:
        st.session_state.current_model_path = ""
    if "current_model_is_sdxl" not in st.session_state:
        st.session_state.current_model_is_sdxl = False
    if "active_loras" not in st.session_state:
        st.session_state.active_loras = {}
    if "generated_images" not in st.session_state:
        st.session_state.generated_images = []
    if "status_message" not in st.session_state:
        st.session_state.status_message = ""
    if "hf_token" not in st.session_state:
        st.session_state.hf_token = HF_TOKEN
    if "civit_token" not in st.session_state:
        st.session_state.civit_token = CIVIT_TOKEN
    if "hf_search_results" not in st.session_state:
        st.session_state.hf_search_results = []
    if "history_loaded" not in st.session_state:
        st.session_state.history_loaded = False

# ============== åœ–ç‰‡æ­·å²è¨˜éŒ„ ==============
def save_to_history(image, prompt, neg_prompt, seed, steps, cfg, width, height, model_path):
    """å„²å­˜åœ–ç‰‡åˆ°æ­·å²è¨˜éŒ„"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{timestamp}_seed{seed}.png"
    filepath = os.path.join(HISTORY_DIR, filename)
    
    # å„²å­˜åœ–ç‰‡
    image.save(filepath)
    
    # å„²å­˜å…ƒè³‡æ–™
    metadata = {
        "timestamp": timestamp,
        "prompt": prompt,
        "negative_prompt": neg_prompt,
        "seed": seed,
        "steps": steps,
        "cfg": cfg,
        "width": width,
        "height": height,
        "model": model_path,
        "filename": filename
    }
    
    json_path = filepath.replace(".png", ".json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    return filepath

def load_history():
    """è¼‰å…¥æ­·å²è¨˜éŒ„"""
    history = []
    json_files = sorted(glob.glob(os.path.join(HISTORY_DIR, "*.json")), reverse=True)
    
    for json_file in json_files[:MAX_HISTORY_IMAGES]:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)
            image_path = json_file.replace(".json", ".png")
            if os.path.exists(image_path):
                metadata["image_path"] = image_path
                history.append(metadata)
        except:
            continue
    
    return history

# ============== HuggingFace æ¨¡å‹æœå°‹ ==============
def search_hf_models(query, limit=10):
    """æœå°‹ HuggingFace ä¸Šçš„ Stable Diffusion æ¨¡å‹"""
    try:
        api = HfApi()
        models = list_models(
            search=query,
            filter="text-to-image",
            limit=limit,
            token=st.session_state.hf_token if st.session_state.hf_token else None
        )
        results = []
        for model in models:
            results.append({
                "id": model.id,
                "downloads": model.downloads,
                "likes": model.likes,
                "tags": model.tags if model.tags else []
            })
        return results
    except Exception as e:
        st.error(f"æœå°‹å¤±æ•—: {str(e)}")
        return []

def download_hf_model(model_id):
    """ä¸‹è¼‰ HuggingFace æ¨¡å‹åˆ°æœ¬åœ°å¿«å–"""
    try:
        with st.spinner(f"ä¸‹è¼‰æ¨¡å‹ {model_id} ä¸­..."):
            local_path = snapshot_download(
                repo_id=model_id,
                cache_dir=MODEL_CACHE_DIR,
                token=st.session_state.hf_token if st.session_state.hf_token else None
            )
        return f"âœ… æ¨¡å‹å·²ä¸‹è¼‰è‡³: {local_path}\næ‚¨å¯ä»¥åœ¨ã€Œæœ¬åœ°æ¨¡å‹ã€ä¸­è¼‰å…¥æ­¤æ¨¡å‹"
    except Exception as e:
        return f"âŒ ä¸‹è¼‰å¤±æ•—: {str(e)}"

# ============== ä¸‹è¼‰å‡½æ•¸ ==============
def download_and_backup(url, folder, civit_token="", hf_token=""):
    """ä¸‹è¼‰æª”æ¡ˆä¸¦å‚™ä»½"""
    headers = {}
    if "civitai.com" in url and civit_token:
        headers["Authorization"] = f"Bearer {civit_token}"
    
    response = requests.get(url, headers=headers, stream=True, timeout=60)
    response.raise_for_status()
    
    # å˜—è©¦å¾ header å–å¾—æª”å
    content_disp = response.headers.get("content-disposition", "")
    filename = "downloaded_model.safetensors"
    if "filename=" in content_disp:
        filename = content_disp.split("filename=")[1].strip('"')
    
    local_filepath = os.path.join(folder, filename)
    
    # ä¸‹è¼‰é€²åº¦
    total_size = int(response.headers.get("content-length", 0))
    progress_bar = st.progress(0, text=f"ä¸‹è¼‰ä¸­: {filename}")
    
    with open(local_filepath, "wb") as f:
        downloaded = 0
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                downloaded += len(chunk)
                if total_size > 0:
                    progress = downloaded / total_size
                    progress_bar.progress(progress, text=f"ä¸‹è¼‰ä¸­: {filename} ({int(progress*100)}%)")
    
    progress_bar.empty()
    return local_filepath, filename, "âœ… ä¸‹è¼‰å®Œæˆ"

# ============== è¼‰å…¥æ¨¡å‹ ==============
@st.cache_resource
def load_pipeline_cached(model_source, is_local_file=False, hf_token=""):
    """å¿«å–æ¨¡å‹è¼‰å…¥"""
    is_sdxl = False
    
    if is_local_file:
        if "sdxl" in model_source.lower() or "xl" in model_source.lower():
            pipe = StableDiffusionXLPipeline.from_single_file(
                model_source,
                torch_dtype=torch.float32,
                use_safetensors=True,
            )
            is_sdxl = True
        else:
            pipe = StableDiffusionPipeline.from_single_file(
                model_source,
                torch_dtype=torch.float32,
                use_safetensors=True,
            )
    else:
        # åˆ¤æ–·æ˜¯å¦ç‚º SDXL
        if "sdxl" in model_source.lower() or "xl" in model_source.lower():
            pipe = StableDiffusionXLPipeline.from_pretrained(
                model_source,
                torch_dtype=torch.float32,
                use_auth_token=hf_token if hf_token else None,
            )
            is_sdxl = True
        else:
            pipe = StableDiffusionPipeline.from_pretrained(
                model_source,
                torch_dtype=torch.float32,
                use_auth_token=hf_token if hf_token else None,
            )
    
    pipe.to("cpu")
    pipe.safety_checker = None
    pipe.requires_safety_checker = False
    
    # å„ªåŒ–ç®¡ç·šä»¥é©æ‡‰ CPU/ä½è¨˜æ†¶é«”ç’°å¢ƒ
    pipe = optimize_pipeline(pipe)

    return pipe, model_source, is_sdxl

def load_pipeline(model_source, is_local_file=False):
    """è¼‰å…¥æ¨¡å‹ç®¡ç·š"""
    try:
        pipe, path, is_sdxl = load_pipeline_cached(
            model_source, 
            is_local_file, 
            st.session_state.hf_token
        )
        st.session_state.pipe = pipe
        st.session_state.current_model_path = path
        st.session_state.current_model_is_sdxl = is_sdxl
        st.session_state.active_loras = {}
        return f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ: {model_source}"
    except Exception as e:
        return f"âŒ è¼‰å…¥å¤±æ•—: {str(e)}"

# ============== æ¨¡å‹è™•ç† ==============
def handle_model_dropdown(choice):
    """è™•ç†é è¨­æ¨¡å‹é¸æ“‡"""
    if not choice:
        return "è«‹é¸æ“‡æ¨¡å‹"
    
    model_id = PRESET_MODELS.get(choice)
    if model_id:
        return load_pipeline(model_id)
    return "âŒ æœªçŸ¥çš„æ¨¡å‹é¸æ“‡"

def handle_hf_file_model(choice):
    """è™•ç† HF æª”æ¡ˆæ¨¡å‹"""
    if not choice:
        return "è«‹é¸æ“‡æ¨¡å‹"
    
    repo_id, filename = HF_FILE_MODELS.get(choice, (None, None))
    if repo_id and filename:
        try:
            filepath = hf_hub_download(
                repo_id=repo_id,
                filename=filename,
                cache_dir=MODEL_CACHE_DIR,
                token=st.session_state.hf_token
            )
            return load_pipeline(filepath, is_local_file=True)
        except Exception as e:
            return f"âŒ ä¸‹è¼‰å¤±æ•—: {str(e)}"
    return "âŒ æœªçŸ¥çš„æ¨¡å‹é¸æ“‡"

def handle_civitai_model_download(url):
    """è™•ç† Civitai æ¨¡å‹ä¸‹è¼‰"""
    if not url:
        return "è«‹è¼¸å…¥ Civitai æ¨¡å‹ URL"
    
    try:
        path, fname, msg = download_and_backup(
            url, MODEL_CACHE_DIR, 
            st.session_state.civit_token, 
            st.session_state.hf_token
        )
        result = load_pipeline(path, is_local_file=True)
        return f"{msg}\n{result}"
    except Exception as e:
        return f"âŒ ä¸‹è¼‰å¤±æ•—: {str(e)}"

# ============== LoRA è™•ç† ==============
def get_available_loras():
    """å–å¾—å¯ç”¨çš„ LoRA åˆ—è¡¨"""
    loras = []
    for folder in [LORA_CACHE_DIR, "./custom_loras"]:
        if os.path.exists(folder):
            for f in os.listdir(folder):
                if f.endswith((".safetensors", ".bin")):
                    loras.append(os.path.join(folder, f))
    return loras

def handle_lora_dropdown(lora_path, scale):
    """è™•ç† LoRA è¼‰å…¥"""
    if not lora_path or not os.path.exists(lora_path):
        return "è«‹é¸æ“‡æœ‰æ•ˆçš„ LoRA"
    
    if st.session_state.pipe is None:
        return "âŒ è«‹å…ˆè¼‰å…¥æ¨¡å‹"
    
    try:
        st.session_state.pipe.load_lora_weights(
            os.path.dirname(lora_path),
            weight_name=os.path.basename(lora_path),
            cross_attention_scale=scale
        )
        st.session_state.active_loras[lora_path] = scale
        return f"âœ… LoRA è¼‰å…¥æˆåŠŸ: {os.path.basename(lora_path)}"
    except Exception as e:
        return f"âŒ LoRA è¼‰å…¥å¤±æ•—: {str(e)}"

def handle_lora_download(url, scale):
    """è™•ç† LoRA ä¸‹è¼‰"""
    if not url:
        return "è«‹è¼¸å…¥ LoRA URL"
    
    if st.session_state.pipe is None:
        return "âŒ è«‹å…ˆè¼‰å…¥æ¨¡å‹"
    
    try:
        path, fname, msg = download_and_backup(
            url, LORA_CACHE_DIR,
            st.session_state.civit_token,
            st.session_state.hf_token
        )
        result = handle_lora_dropdown(path, scale)
        return f"{msg}\n{result}"
    except Exception as e:
        return f"âŒ ä¸‹è¼‰å¤±æ•—: {str(e)}"

def clear_loras():
    """æ¸…é™¤æ‰€æœ‰ LoRA"""
    if st.session_state.pipe is None:
        return "æ²’æœ‰è¼‰å…¥çš„æ¨¡å‹"
    
    try:
        st.session_state.pipe.unload_lora_weights()
        st.session_state.active_loras = {}
        return "âœ… å·²æ¸…é™¤æ‰€æœ‰ LoRA"
    except Exception as e:
        return f"âŒ æ¸…é™¤å¤±æ•—: {str(e)}"

# ============== åœ–ç‰‡ç”Ÿæˆ ==============
def generate_image(prompt, neg_prompt, steps, cfg, seed, width, height):
    """ç”Ÿæˆåœ–ç‰‡"""
    if st.session_state.pipe is None:
        return None, "âŒ è«‹å…ˆè¼‰å…¥æ¨¡å‹"

    try:
        # æ¸…ç†è¨˜æ†¶é«”
        clear_memory()
        
        # è¨­å®š scheduler (ä½¿ç”¨ DPMSolverMultistepScheduler)
        st.session_state.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
            st.session_state.pipe.scheduler.config
        )

        # ç”Ÿæˆåƒæ•¸
        generator = torch.Generator("cpu").manual_seed(seed)

        # åŸ·è¡Œç”Ÿæˆ
        with st.spinner("ç”Ÿæˆä¸­..."):
            result = st.session_state.pipe(
                prompt=prompt,
                negative_prompt=neg_prompt,
                num_inference_steps=steps,
                guidance_scale=cfg,
                width=width,
                height=height,
                generator=generator,
            )

        image = result.images[0]

        # å„²å­˜åˆ°æ­·å²è¨˜éŒ„ (åŒ…å«å®Œæ•´å…ƒè³‡æ–™)
        history_path = save_to_history(
            image, prompt, neg_prompt, seed, steps, cfg,
            width, height, st.session_state.current_model_path
        )

        # å„²å­˜åˆ° outputs è³‡æ–™å¤¾ (ç°¡å–®å‚™ä»½)
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_path = os.path.join(OUTPUT_DIR, f"image_{timestamp}_seed{seed}.png")
        image.save(save_path)

        # åŠ å…¥åˆ°å·²ç”Ÿæˆåˆ—è¡¨
        st.session_state.generated_images.append({
            "image": image,
            "path": save_path,
            "history_path": history_path,
            "prompt": prompt,
            "seed": seed,
            "steps": steps,
            "cfg": cfg,
            "width": width,
            "height": height
        })

        # ç”Ÿæˆå¾Œæ¸…ç†è¨˜æ†¶é«”
        clear_memory()

        return image, f"âœ… ç”Ÿæˆå®Œæˆ! å·²å„²å­˜è‡³ {save_path}"

    except Exception as e:
        clear_memory()
        return None, f"âŒ ç”Ÿæˆå¤±æ•—: {str(e)}"

# ============== ä¸»ä»‹é¢ ==============
def main():
    st.set_page_config(
        page_title="Turbo CPU SD + æ°¸ä¹…åœ–åº«",
        page_icon="âš¡",
        layout="wide"
    )
    
    # åˆå§‹åŒ– session state
    init_session_state()
    
    # æ¨™é¡Œ
    st.title("âš¡ Turbo CPU SD (NSFW + SDXL/Pony æ”¯æ´)")
    st.markdown("---")
    
    # å´é‚Šæ¬„ - æˆæ¬Šè¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ æˆæ¬Šé‡‘é‘°è¨­å®š")
        st.session_state.hf_token = st.text_input(
            "HF Token", 
            value=st.session_state.hf_token,
            type="password"
        )
        st.session_state.civit_token = st.text_input(
            "Civitai Token", 
            value=st.session_state.civit_token,
            type="password"
        )
        
        st.markdown("---")
        st.header("ğŸ“Š ç‹€æ…‹")
        if st.session_state.current_model_path:
            st.success(f"ç›®å‰æ¨¡å‹: {st.session_state.current_model_path}")
        else:
            st.warning("å°šæœªè¼‰å…¥æ¨¡å‹")

        if st.session_state.active_loras:
            st.info(f"å·²è¼‰å…¥ LoRA: {len(st.session_state.active_loras)} å€‹")

        st.markdown("---")
        st.header("ğŸ–¼ï¸ æœ€è¿‘ç”Ÿæˆ")
        if st.session_state.generated_images:
            for img_data in reversed(st.session_state.generated_images[-3:]):
                st.image(img_data["image"], caption=f"Seed: {img_data['seed']}", use_container_width=True)
        else:
            st.write("å°šç„¡ç”Ÿæˆçš„åœ–ç‰‡")
        
        # æ­·å²è¨˜éŒ„é€£çµ
        st.markdown("---")
        st.header("ğŸ“š æ­·å²è¨˜éŒ„")
        history_count = len(glob.glob(os.path.join(HISTORY_DIR, "*.json")))
        st.write(f"å·²å„²å­˜ {history_count} å¼µåœ–ç‰‡")
    
    # ä¸»è¦å…§å®¹å€
    col_left, col_right = st.columns([1, 2])
    
    # å·¦å´ - æ¨¡å‹èˆ‡ LoRA æ§åˆ¶
    with col_left:
        # æ¨¡å‹é¸æ“‡ Tabs
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“¦ é è¨­æ¨¡å‹", "ğŸ” HF æœå°‹", "ğŸ“ HF æª”æ¡ˆæ¨¡å‹", "ğŸ”— Civitai ä¸‹è¼‰"])

        with tab1:
            st.subheader("é è¨­æ¨¡å‹")
            preset_choice = st.selectbox(
                "é¸æ“‡é è¨­æ¨¡å‹",
                options=[""] + list(PRESET_MODELS.keys()),
                key="preset_model_select"
            )
            if st.button("è¼‰å…¥é è¨­æ¨¡å‹", key="load_preset"):
                with st.spinner("è¼‰å…¥ä¸­..."):
                    st.session_state.status_message = handle_model_dropdown(preset_choice)

        with tab2:
            st.subheader("HuggingFace æ¨¡å‹æœå°‹")
            hf_search_query = st.text_input("æœå°‹æ¨¡å‹", placeholder="è¼¸å…¥é—œéµå­—æœå°‹...", key="hf_search_query")
            if st.button("ğŸ” æœå°‹", key="search_hf"):
                if hf_search_query:
                    with st.spinner("æœå°‹ä¸­..."):
                        st.session_state.hf_search_results = search_hf_models(hf_search_query)

            if st.session_state.hf_search_results:
                st.write(f"æ‰¾åˆ° {len(st.session_state.hf_search_results)} å€‹æ¨¡å‹:")
                for model in st.session_state.hf_search_results[:5]:
                    with st.container():
                        st.write(f"**{model['id']}**")
                        st.caption(f"â¬‡ï¸ {model['downloads']:,} | â¤ï¸ {model['likes']}")
                        
                        col_m1, col_m2, col_m3 = st.columns(3)
                        with col_m1:
                            if st.button("ğŸ“¥ ä¸‹è¼‰", key=f"dl_{model['id'].replace('/', '_')}"):
                                st.session_state.status_message = download_hf_model(model['id'])
                        with col_m2:
                            if st.button("âš¡ ç›´æ¥è¼‰å…¥", key=f"load_{model['id'].replace('/', '_')}"):
                                st.session_state.status_message = load_pipeline(model['id'])
                        with col_m3:
                            # é¡¯ç¤ºæ¨¡å‹é é¢é€£çµ
                            st.link_button("ğŸ”— é–‹å•Ÿé é¢", f"https://huggingface.co/{model['id']}")
                        st.markdown("---")

        with tab3:
            st.subheader("HF æª”æ¡ˆæ¨¡å‹")
            hf_choice = st.selectbox(
                "é¸æ“‡ HF æª”æ¡ˆæ¨¡å‹",
                options=[""] + list(HF_FILE_MODELS.keys()),
                key="hf_model_select"
            )
            if st.button("è¼‰å…¥ HF æ¨¡å‹", key="load_hf"):
                with st.spinner("è¼‰å…¥ä¸­..."):
                    st.session_state.status_message = handle_hf_file_model(hf_choice)

        with tab4:
            st.subheader("Civitai ä¸‹è¼‰")
            civit_url = st.text_input("Civitai æ¨¡å‹ URL", key="civit_url")
            if st.button("ä¸‹è¼‰ä¸¦è¼‰å…¥", key="download_civit"):
                if civit_url:
                    st.session_state.status_message = handle_civitai_model_download(civit_url)
        
        st.markdown("---")
        
        # LoRA æ§åˆ¶
        st.subheader("ğŸ¨ LoRA æ§åˆ¶")
        
        lora_tabs1, lora_tabs2 = st.tabs(["æœ¬åœ° LoRA", "ä¸‹è¼‰ LoRA"])
        
        with lora_tabs1:
            available_loras = get_available_loras()
            lora_choice = st.selectbox(
                "é¸æ“‡æœ¬åœ° LoRA",
                options=[""] + available_loras,
                format_func=lambda x: os.path.basename(x) if x else "",
                key="lora_select"
            )
            lora_scale = st.slider("LoRA å¼·åº¦", 0.0, 2.0, 1.0, 0.1, key="lora_scale")
            if st.button("è¼‰å…¥ LoRA", key="load_lora"):
                st.session_state.status_message = handle_lora_dropdown(lora_choice, lora_scale)
        
        with lora_tabs2:
            lora_url = st.text_input("LoRA URL", key="lora_url")
            lora_dl_scale = st.slider("ä¸‹è¼‰ LoRA å¼·åº¦", 0.0, 2.0, 1.0, 0.1, key="lora_dl_scale")
            if st.button("ä¸‹è¼‰ä¸¦è¼‰å…¥ LoRA", key="download_lora"):
                st.session_state.status_message = handle_lora_download(lora_url, lora_dl_scale)
        
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰ LoRA", key="clear_loras"):
            st.session_state.status_message = clear_loras()
        
        # é¡¯ç¤ºç‹€æ…‹è¨Šæ¯
        if st.session_state.status_message:
            st.markdown("---")
            st.info(st.session_state.status_message)
    
    # å³å´ - ç”Ÿæˆæ§åˆ¶
    with col_right:
        st.subheader("ğŸ–¼ï¸ åœ–ç‰‡ç”Ÿæˆ")
        
        # æç¤ºè©
        prompt = st.text_area(
            "æ­£å‘æç¤ºè© (Prompt)",
            height=100,
            placeholder="è¼¸å…¥æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„åœ–ç‰‡å…§å®¹..."
        )
        
        neg_prompt = st.text_area(
            "è² å‘æç¤ºè© (Negative Prompt)",
            value="lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry",
            height=80
        )
        
        # ç”Ÿæˆåƒæ•¸
        col_param1, col_param2, col_param3 = st.columns(3)
        
        with col_param1:
            steps = st.slider("æ­¥æ•¸ (Steps)", 1, 50, 20, key="steps")
            cfg = st.slider("CFG Scale", 1.0, 20.0, 7.0, 0.5, key="cfg")
        
        with col_param2:
            width = st.select_slider(
                "å¯¬åº¦",
                options=[384, 512, 640, 768, 896, 1024, 1152, 1280],
                value=512,
                key="width"
            )
            height = st.select_slider(
                "é«˜åº¦",
                options=[384, 512, 640, 768, 896, 1024, 1152, 1280],
                value=512,
                key="height"
            )
        
        with col_param3:
            seed = st.number_input("ç¨®å­ (Seed)", -1, 999999999, -1, key="seed")
            if seed == -1:
                import random
                seed = random.randint(0, 999999999)

        # ç”ŸæˆæŒ‰éˆ•
        if st.button("ğŸ¨ ç”Ÿæˆåœ–ç‰‡", type="primary", use_container_width=True):
            if not prompt:
                st.error("è«‹è¼¸å…¥æç¤ºè©")
            else:
                image, message = generate_image(
                    prompt, neg_prompt, steps, cfg, seed, width, height
                )
                if image:
                    st.success(message)
                    st.image(image, caption=f"Seed: {seed}", use_container_width=True)
                else:
                    st.error(message)
        
        # é¡¯ç¤ºå·²ç”Ÿæˆçš„åœ–ç‰‡
        if st.session_state.generated_images:
            st.markdown("---")
            st.subheader("ğŸ“¸ å·²ç”Ÿæˆåœ–ç‰‡")
            for img_data in reversed(st.session_state.generated_images):
                with st.expander(f"Seed: {img_data['seed']} - {img_data['prompt'][:50]}..."):
                    st.image(img_data["image"], use_container_width=True)
                    st.caption(f"æç¤ºè©: {img_data['prompt']}")

if __name__ == "__main__":
    main()
